# inspector_app.py
import streamlit as st
import json
import os
import pandas as pd
from collections import defaultdict

# Import library ‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡πÄ‡∏£‡∏≤
from pipeline_lib.config_loader import load_config
from pipeline_lib.db_handler import get_db_connection

# --- ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ---
CONFIG_PATH = "config.yaml"

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏° ---

@st.cache_data # Cache a data loading to improve performance
def load_faiss_metadata(filepath):
    """Loads the Faiss metadata JSON file."""
    if not os.path.exists(filepath):
        return None
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

@st.cache_resource # Cache DB connection
def get_cached_db_connection(db_config):
    """Gets a cached database connection."""
    return get_db_connection(db_config)

def execute_query(conn, query, params=None, fetch="all"):
    """Executes a SQL query and returns the result."""
    try:
        with conn.cursor() as cur:
            cur.execute(query, params)
            if fetch == "one":
                return cur.fetchone()
            else:
                return cur.fetchall()
    except Exception as e:
        st.error(f"Database query failed: {e}")
        return None

def display_chunks(title, chunks_data):
    """A reusable function to display chunks in expanders with length info."""
    if title and chunks_data:
        st.success(f"**‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:** {title}")
        st.write(f"‡∏û‡∏ö **{len(chunks_data)}** Chunks:")
        
        for i, chunk in enumerate(chunks_data):
            seq = chunk.get('chunk_sequence') if isinstance(chunk, dict) else chunk[0]
            text = chunk.get('chunk_text') if isinstance(chunk, dict) else chunk[1]
            
            # --- START: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ---
            # 1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á chunk_text
            chunk_length = len(text)
            
            # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß
            expander_title = f"Chunk #{seq} (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß: {chunk_length} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)"
            
            # 3. ‡πÉ‡∏ä‡πâ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏°‡πà‡∏Å‡∏±‡∏ö st.expander
            with st.expander(expander_title):
            # --- END: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ---
                st.text_area(f"Chunk Content {i+1}", text, height=200, disabled=True)
                if isinstance(chunk, dict) and 'metadata' in chunk:
                    st.json(chunk['metadata'])

    elif title:
        st.warning(f"**‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:** {title}\n\n‡πÑ‡∏°‡πà‡∏û‡∏ö Chunks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ")
    else:
        st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ID ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏")

# --- ‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÅ‡∏≠‡∏õ ---

st.set_page_config(layout="wide", page_title="Index Inspector")
st.title("üî¨ Universal Index Inspector")
st.markdown("---")

# 1. ‡πÇ‡∏´‡∏•‡∏î Config ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏°‡∏î‡πÑ‡∏´‡∏ô
config = load_config(CONFIG_PATH)
if not config:
    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå config.yaml ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö")
    st.stop()

store_type = config.get('vector_store', {}).get('type', 'PGVECTOR')
st.info(f"‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: **{store_type}** (‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å config.yaml)")

# ===================================================================
# ‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å PostgreSQL (PGVECTOR)
# ===================================================================
if store_type == 'PGVECTOR':
    st.header("Inspecting from PostgreSQL")
    conn = get_cached_db_connection(config['database'])
    if conn:
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
        st.subheader("‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏ô `knowledge_items`")
        items_df = pd.read_sql("SELECT id, title, source_type, status FROM knowledge_items ORDER BY id", conn)
        st.dataframe(items_df, use_container_width=True, hide_index=True)

        # ‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏π Chunks
        st.divider()
        st.subheader("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πà‡∏≠‡∏¢ (Chunk Viewer)")
        
        item_id_to_view = st.number_input(
            "‡πÉ‡∏™‡πà ID ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π", min_value=1, step=1,
            help="‡∏î‡∏π ID ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô"
        )
        
        if st.button("üî¨ ‡πÅ‡∏™‡∏î‡∏á Chunks", use_container_width=True):
            if item_id_to_view > 0:
                parent_item = execute_query(conn, "SELECT title FROM knowledge_items WHERE id = %s", (item_id_to_view,), fetch="one")
                chunks = execute_query(conn, "SELECT chunk_sequence, chunk_text FROM knowledge_chunks WHERE knowledge_item_id = %s ORDER BY chunk_sequence", (item_id_to_view,), fetch="all")
                display_chunks(parent_item[0] if parent_item else None, chunks)

# ===================================================================
# ‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å Faiss (‡πÑ‡∏ü‡∏•‡πå .bin ‡πÅ‡∏•‡∏∞ .json)
# ===================================================================
elif store_type == 'FAISS':
    st.header("Inspecting from Faiss Files")
    metadata_path = config.get('vector_store', {}).get('faiss', {}).get('metadata_path', '')
    
    records = load_faiss_metadata(metadata_path)
    if records:
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        doc_summary = defaultdict(lambda: {'title': 'N/A', 'chunk_count': 0, 'chunks': []})
        for i, record in enumerate(records):
            doc_id = record['metadata'].get('document_id')
            if doc_id:
                doc_summary[doc_id]['title'] = record['metadata'].get('document_title', 'Title not found')
                doc_summary[doc_id]['chunk_count'] += 1
                record_for_display = record.copy()
                record_for_display['chunk_sequence'] = record['metadata'].get('chunk_sequence', i)
                doc_summary[doc_id]['chunks'].append(record_for_display)

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
        st.subheader("‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô `metadata.json`")
        summary_df = pd.DataFrame([
            {"id": doc_id, "title": data['title'], "chunk_count": data['chunk_count']}
            for doc_id, data in doc_summary.items()
        ]).sort_values(by="id")
        st.dataframe(summary_df, use_container_width=True, hide_index=True)

        # ‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏π Chunks
        st.divider()
        st.subheader("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πà‡∏≠‡∏¢ (Chunk Viewer)")
        
        item_id_to_view = st.number_input(
            "‡πÉ‡∏™‡πà ID ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π", min_value=1, step=1,
            help="‡∏î‡∏π ID ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô"
        )

        if st.button("üî¨ ‡πÅ‡∏™‡∏î‡∏á Chunks", use_container_width=True):
            if item_id_to_view in doc_summary:
                doc_data = doc_summary[item_id_to_view]
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö chunk ‡∏ï‡∏≤‡∏° sequence ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
                sorted_chunks = sorted(doc_data['chunks'], key=lambda x: x['chunk_sequence'])
                display_chunks(doc_data['title'], sorted_chunks)
            else:
                st.error(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ID: {item_id_to_view} ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå metadata")